# **Bio-Aware Blended Spaces (BABS): A Comprehensive Multi-Phase Meta-Project Proposal**

## **Introductory Summary**
Bio-Aware Blended Spaces (BABS) is an integrative research and engineering system designed to explore how biosensory data can be transformed into responsive, immersive environments through structured mediation pathways. Built upon the Balanced Blended Space (BBS) framework and realized through the Blended Reality Performance System (BRPS), BABS bridges the gap between human physiological expression and computational responsiveness. 

This system demonstrates effectiveness by allowing cognitive agents (humans) and computational agents (AI systems) to co-experience and co-shape shared environments. Physiological data—such as heart rate, EEG activity, or movement—is captured through biosensors, normalized through middleware protocols (LSL, OSC, BLE), and translated into audiovisual or kinetic responses. Each transformation forms a defined mediation pathway, ensuring transparent mapping between biological input, digital processing, and environmental feedback. 

The long-term effectiveness of BABS lies in its scalability: from early prototypes using a single sensor, to complex, distributed BRPS deployments involving multiple participants, AI interpretation, and adaptive environments. Through this progressive development, BABS seeks to formalize a language of interaction where both human and machine intelligences can engage symmetrically in creative, performative, and therapeutic contexts.
 **Bio-Aware Blended Spaces (BABS): A Comprehensive Multi-Phase Meta-Project Proposal**

## **Project Phases Overview**
The Bio-Aware Blended Spaces (BABS) meta-project unfolds across four progressive phases, each expanding the research, complexity, and collaborative scope of the system:
- **Phase 1:** Foundational research and proof-of-concept testing using a single or small set of biosensors to verify transformation of biological data into audiovisual mediation.
- **Phase 2:** Introduction of recursive feedback systems and AI symmetry, allowing adaptive environmental response and emergent behaviors.
- **Phase 3:** Full BRPS physical deployment, integrating robotics, spatialized sound, and projection-mapped architecture.
- **Phase 4:** Applied extensions into performance, therapy, education, and distributed collaboration.

Each phase builds upon the previous one, evolving from technical validation to experiential application while maintaining fidelity to the BBS framework’s balance between cognitive and computational agents.

## **System Mediation Diagram (Conceptual Overview)**
Using **BBS mediation pathway syntax**, BABS models data and perception flow as a dynamic network of transformations:

**Cognitive Agent (Human)** —(biosignal acquisition via sensors)→ **Mediation Pathway 1: Capture Layer** —(LSL/OSC normalization)→ **Mediation Pathway 2: Data Layer** —(signal mapping via Max/MSP + Jitter)→ **Mediation Pathway 3: Transformative Layer (Audio/Visual/Physical Output)** —(feedback from environment)→ **Cognitive Agent**.

**Computational Agent (AI)** —(receives normalized biosignal data)→ **Mediation Pathway 4: Interpretation Layer (Machine Learning)** —(responds or modifies system)→ **Mediation Pathway 5: Recursive Feedback** —(informs both human and AI states)→ **Blended Environment**.

Together, these mediation pathways form a balanced circuit of perception, transformation, and reflection between physical and virtual domains. **Bio-Aware Blended Spaces (BABS): A BBS Meta-Project Proposal**

## **Meta-Project Context**
Bio-Aware Blended Spaces (BABS) is a **type of Blended Reality Performance System (BRPS) designed around the Balanced Blended Space (BBS) framework**. As a meta-project, BABS extends BRPS’s role as a modular testbed where live physical, virtual, and conceptual components interact. Like the Blended Shadow Puppet project, BABS explores how these realities interconnect symmetrically, with **cognitive (human)** and **computational (AI)** intelligences treated as **collaborative partners** rather than hierarchically arranged.

BBS emphasizes:
- **Physical/Virtual Symmetry** (atoms ↔ numbers).
- **Cognitive/Computational Symmetry** (biological ↔ machine intelligence).
- **Mediation Pathway Symmetry** (direct ↔ mediated experiences).
- **Sensory Modality Symmetry** (vision, sound, touch, bio-signals, etc.).
- **Space-Time Symmetry** (real-time, latent, recorded, inaccessible).

BABS integrates these symmetries into **immersive environments** where human biosignals and AI-driven **bio-aware blended spaces** co-shape shared experiences through carefully designed **mediation pathways**.

---

## **Project Overview**
BABS explores **biosensing and mapping** as mediation pathways between humans, AI, and blended immersive spaces. Unlike metaverse approaches that isolate users inside VR headsets【37†source】, BABS emphasizes **shared, blended spaces** (projection-mapped rooms, sound arrays, robotic set pieces). In keeping with BBS principles, the **physical environment is central for cognitive agents**, while the **virtual environment is central for computational agents**, ensuring balanced mediation between the two.

### **Core Research Questions**
1. How can biosignals (EEG, HRV, GSR, respiration, motion) be transformed into numerical domains for real-time mapping into blended environments through mediation pathways?
2. What recursive feedback loops emerge when environments influence biosignals, which in turn reshape those environments via new mediation pathways?
3. What happens when AI participates through its own **bio-aware blended spaces**, allowing it to co-experience and modify the environment symmetrically? 
4. How can disruption (negative feedback) be used creatively to destabilize or evolve mediated systems?
5. How do these mappings scale from individual → collective → distributed immersive mediation pathways?

---

## **Project Stages & Subprojects**

### **Stage 1: Core Lab Prototypes (Foundations)**
1. **Biosensor Acquisition Module**  
   - Devices: OpenBCI, Muse, Polar H10, Apple Watch, Fitbit, Empatica E4.  
   - Primitive sensors: contact microphones for heartbeat, piezo elements, simple pressure pads.  
   - Cameras: standard webcams, depth cameras (Kinect, Intel RealSense) to detect respiration, micro-movements, or facial expressions.  
   - Middleware: Lab Streaming Layer (LSL), OSC, BLE gateways.  
   - Goal: Normalize data into streams usable across artistic/technical platforms as inputs to mediation pathways.

2. **Max/MSP + Jitter Integration**  
   - Map biosignals into basic audiovisual transformations.  
   - Example: heart rate → rhythmic pulses; EEG alpha waves → color gradients.  
   - Demonstrates first-generation mediation pathways from biosignal to environment.

3. **Projection Sandbox (Single Surface)**  
   - Small-scale projection testbed for direct mappings.  
   - Demonstrates bio → environment → perception loop as a mediation pathway.

**Stage 1 Deliverables:**
- A working biosensor pipeline (including consumer wearables, primitive sensors, and camera inputs) streaming into LSL/OSC as mediation pathways.  
- At least one Max/MSP patch demonstrating audio or visual mapping of a biosignal through mediation.  
- A single-surface projection prototype showing real-time bio → environment transformations as mediation pathways.  
- Documentation of sensor calibration, data normalization methods, and mapping strategies framed as mediation design.  
- A preliminary technical guide for scaling Stage 1 mediation pathways into BRPS environments.

---

### **Stage 2: Recursive Feedback & AI Symmetry (Experimental)**
4. **Recursive Feedback Engine**  
   - Environments designed to calm or destabilize bio states, then adapt accordingly via recursive mediation pathways.

5. **Disruption/Negative Feedback Module**  
   - Algorithms introduce inversions or instability to study emergent behaviors of mediation pathways.

6. **Machine Learning Mapper**  
   - Classifies biosignals into states (focus, relaxation, arousal).  
   - Learns participant-specific profiles to refine mediation pathways.

7. **AI Bio-Aware Blended Space Prototype**  
   - Define computational equivalents to biosensors (e.g., AI uncertainty, entropy).  
   - Test AI as a co-experiencer through its own bio-aware blended spaces rather than as a passive interpreter.

---

### **Stage 3: BRPS Physical Deployment (Immersive Blended Space)**
8. **Immersive Projection Environments**  
   - BRPS installations: CAVE-like rooms, domes, tensile fabric surfaces【39†source】.  
   - Projection mapping responds in real time to biosignal inputs and video-derived signals as part of physical-virtual mediation pathways.

9. **Spatialized Sound + Virtual Orchestra**  
   - Multi-channel speaker arrays.  
   - Biosignals drive orchestration dynamics, timbre, or ensemble participation via sound mediation pathways【38†source】.

10. **Robotic & Kinetic Interfaces**  
   - Servo-driven puppets, robotic arms, moving set pieces.  
   - Biosignals reshape physical environment directly as tangible mediation pathways.

11. **Multi-Participant Shared Space**  
   - Collective biosensor inputs shape shared projections, sound, and robotics.  
   - Includes camera-based group analysis (movement, rhythm, emotional state).  
   - Tests collective intelligence feedback loops through multi-agent mediation pathways.

---

### **Stage 4: Applied Extensions (Parallel Branches)**
**Artistic Applications**
12. **Collaborative Music Engine** – Biosignal → MIDI control for Ableton, Cubase via mediation pathways.  
13. **Vocal & Effects Mapping** – Bio-influenced vocal processing (BandLab, VST plugins) as auditory mediation pathways.  
14. **Distributed Performance Network** – OSC links between remote BRPS nodes, extending mediation pathways across distance.

**Therapeutic/Educational Applications**
15. **Therapeutic Sandbox** – Biofeedback-driven calming environments as therapeutic mediation pathways.  
16. **Narrative Integration** – Branching stories shaped by participant affective states, guided through narrative mediation pathways.  
17. **LLM Environment Companion** – AI trained on biosignal logs, co-creates environments, offering generative mediation pathways.

---

## **Technology Integration**
- **Consumer wearables** (Apple Watch, Fitbit, Oura) → scalable audience/participant data for mediation pathways.  
- **Research devices** (OpenBCI, Empatica E4, Polar H10) → high-fidelity anchors for precise mediation pathways.  
- **Primitive sensors** (contact mics, piezo elements, pressure mats) → low-cost, experimental mediation inputs.  
- **Cameras** (RGB + depth) → dual role: capture environmental context + infer biometric states as mediation pathways.  
- **Middleware**: LSL for sync, OSC for creative mapping, Node-RED/MQTT for distributed mediation pathways.  
- **Platforms**: Max/MSP Jitter, TouchDesigner, QLab, Unity/Unreal (non-headset projection).  
- **Deployment**: BRPS immersive environments with projection, robotics, sound arrays as blended mediation systems.

---

## **Expected Outcomes**
- A validated **bio–AI co-experience pipeline** within the BBS framework as a set of mediation pathways.  
- Demonstrations of **recursive mediation pathways** (bio ↔ environment ↔ bio).  
- Prototype **bio-aware blended spaces** for AI participation, defining computational mediation pathways.  
- Artistic performances, therapeutic applications, and educational experiments tested as mediation pathways.  
- Scalable integration of consumer wearables, research devices, primitive sensors, and camera-based mediation inputs.  
- Outcomes evaluated from both perspectives: the **physical mediation pathways central to cognitive agents** and the **virtual mediation pathways central to computational agents**, ensuring balanced validation across intelligences.

---

## **Significance within BBS & CHI**
BABS extends the **Balanced Blended Space framework** by embedding **biosignal-driven mediation pathways** into **BRPS immersive systems**. It demonstrates how **cognitive and computational intelligences** can share a blended stage, aligning with CHI’s holistic, interdisciplinary mission【37†source】【38†source】【39†source】.

This project is designed as a **meta-project**, with subprojects accessible to student researchers, faculty collaborations, and interdisciplinary teams, each contributing to the unified goal of creating **bio-aware blended immersive environments through mediation pathways**.

